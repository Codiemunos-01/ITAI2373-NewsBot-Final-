# Install required packages
!pip install spacy scikit-learn nltk pandas matplotlib seaborn wordcloud plotly
!python -m spacy download en_core_web_sm

# Download NLTK data
import nltk
nltk.download('punkt') #Punkt sentence tokenizer
nltk.download('stopwords') #commonly occuring words filtered out 
nltk.download('wordnet') #lexical database of english
nltk.download('vader_lexicon') #Core resource for Valence Aware Dictionary and sEntiment Reasoner also known as VADER
nltk.download('averaged_perceptron_tagger') #POS tagger based on averaged perceptron algorithm
nltk.download('punkt_tab') #newer version of the Punkt tokenizer
nltk.download('averaged_perceptron_tagger_eng')#POS tagger

# Import all necessary libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from wordcloud import WordCloud
import plotly.express as px
import plotly.graph_objects as go
from collections import Counter, defaultdict
import re
import warnings
warnings.filterwarnings('ignore')

# NLP Libraries
import spacy
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize, sent_tokenize
from nltk.stem import WordNetLemmatizer
from nltk.sentiment import SentimentIntensityAnalyzer
from nltk.tag import pos_tag

# Scikit-learn for machine learning
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.naive_bayes import MultinomialNB
from sklearn.svm import SVC
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from sklearn.pipeline import Pipeline

# Load spaCy model
nlp = spacy.load('en_core_web_sm')

# Set up plotting style
plt.style.use('default')
sns.set_palette("husl")

# Install Kaggle API
%pip install kaggle

#Upload kaggle.json file
from google.colab import userdata
userdata.get('KAGGLE_API_KEY') #used to access the secret API key for Kaggle

#Set up API credentials
!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json

# Download BBC News Dataset
!kaggle competitions download -c learn-ai-bbc

# Load the dataset
import pandas as pd
import os

# Load the main dataset
df = pd.read_csv('BBC News Train.csv')  #filename

# Initialize preprocessing tools
lemmatizer = WordNetLemmatizer()
stop_words = set(stopwords.words('english'))

def clean_text(text):
# Remove URLs
    text = re.sub(r'http\S+|www\S+|https\S+', '', text, flags=re.MULTILINE)
# Initialize preprocessing tools
lemmatizer = WordNetLemmatizer()
stop_words = set(stopwords.words('english'))

def clean_text(text):


    """
    Comprehensive text cleaning function

    """
    if pd.isna(text):
        return ""

    # Convert to string and lowercase
    text = str(text).lower()

    # ðŸš€ Implement text cleaning
    # Remove HTML tags
    text = re.sub(r'<[^>]+>', '', text)

    # Remove URLs
    text = re.sub(r'http\S+|www\S+|https\S+', '', text, flags=re.MULTILINE)

    # Remove email addresses
    text = re.sub(r'\S+@\S+', '', text)

    # Remove special characters and digits (keep only letters and spaces)
    text = re.sub(r'[^a-zA-Z\s]', '', text)

    # Remove extra whitespace
    text = re.sub(r'\s+', ' ', text).strip()

    return text

def preprocess_text(text, remove_stopwords=True, lemmatize=True):
    """
    Complete preprocessing pipeline


    """
    # Clean text
    text = clean_text(text)

    if not text:
        return ""

    # Tokenize
    tokens = word_tokenize(text)

    # Remove stop words if requested
    if remove_stopwords:
        tokens = [token for token in tokens if token not in stop_words]

    # Lemmatize if requested
    if lemmatize:
        tokens = [lemmatizer.lemmatize(token) for token in tokens]

    # Filter out very short words
    tokens = [token for token in tokens if len(token) > 2]

    return ' '.join(tokens)

# Visualize dependency parsing for a sample sentence
  from spacy import displacy

# ðŸŽ­ Advanced Sentiment 
!pip install keybert nrclex transformers spacy nltk matplotlib --quiet
!python -m spacy download xx_ent_wiki_sm --quiet
import math
import pandas as pd
import matplotlib.pyplot as plt
import spacy
from keybert import KeyBERT
from nrclex import NRCLex
from transformers import pipeline, AutoTokenizer
from nltk.sentiment.vader import SentimentIntensityAnalyzer
from datetime import datetime
from typing import List, Dict, Any, Optional
import nltk
nltk.download('vader_lexicon', quiet=True)

**Response Generation!!!**
import google.generativeai as genai
import pandas as pd
from google.colab import userdata

**Topic Modeling and Content Discovery:**
import re
import math
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from typing import List, Dict, Any, Optional, Tuple, Union
from sklearn.decomposition import LatentDirichletAllocation, NMF
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.preprocessing import normalize
from scipy.sparse import csr_matrix
import networkx as nx 
